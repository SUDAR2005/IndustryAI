{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine\n",
    "from pymongo import MongoClient\n",
    "import requests\n",
    "import json\n",
    "from typing import Dict, List, Union\n",
    "import logging\n",
    "\n",
    "class DataPipeline:\n",
    "    def __init__(self, config: Dict):\n",
    "        \"\"\"\n",
    "        Initialize data pipeline with configuration\n",
    "        \n",
    "        config: Dictionary containing:\n",
    "            - database credentials\n",
    "            - API endpoints\n",
    "            - file paths\n",
    "            - data schemas\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.sql_engine = create_engine(config['sql_connection_string'])\n",
    "        self.mongo_client = MongoClient(config['mongo_connection_string'])\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def ingest_structured_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Ingest data from various structured sources\"\"\"\n",
    "        \n",
    "        # 1. Financial Data\n",
    "        financial_data = self._fetch_financial_metrics()\n",
    "        \n",
    "        # 2. ESG Metrics\n",
    "        esg_data = self._fetch_esg_metrics()\n",
    "        \n",
    "        # 3. Climate Data\n",
    "        climate_data = self._fetch_climate_data()\n",
    "        \n",
    "        # Merge all structured data\n",
    "        merged_data = pd.merge(financial_data, esg_data, on='project_id')\n",
    "        merged_data = pd.merge(merged_data, climate_data, on='project_id')\n",
    "        \n",
    "        return merged_data\n",
    "    \n",
    "    def ingest_unstructured_data(self) -> Dict:\n",
    "        \"\"\"Ingest unstructured data like documents and reports\"\"\"\n",
    "        \n",
    "        # 1. Project Documentation\n",
    "        project_docs = self._fetch_project_documents()\n",
    "        \n",
    "        # 2. Sustainability Reports\n",
    "        sustainability_reports = self._fetch_sustainability_reports()\n",
    "        \n",
    "        # 3. News and Media Coverage\n",
    "        news_data = self._fetch_news_data()\n",
    "        \n",
    "        return {\n",
    "            'project_documents': project_docs,\n",
    "            'sustainability_reports': sustainability_reports,\n",
    "            'news_data': news_data\n",
    "        }\n",
    "    \n",
    "    def _fetch_financial_metrics(self) -> pd.DataFrame:\n",
    "        \"\"\"Fetch financial data from various sources\"\"\"\n",
    "        dfs = []\n",
    "        \n",
    "        # Internal database\n",
    "        query = \"\"\"\n",
    "        SELECT project_id, investment_amount, expected_roi, \n",
    "               implementation_cost, maintenance_cost\n",
    "        FROM financial_metrics\n",
    "        WHERE data_date >= NOW() - INTERVAL '1 year'\n",
    "        \"\"\"\n",
    "        internal_data = pd.read_sql(query, self.sql_engine)\n",
    "        dfs.append(internal_data)\n",
    "        \n",
    "        # External APIs (example: Bloomberg, Reuters)\n",
    "        for api_config in self.config['financial_apis']:\n",
    "            response = requests.get(\n",
    "                api_config['endpoint'],\n",
    "                headers={'Authorization': api_config['key']}\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                api_data = pd.DataFrame(response.json())\n",
    "                dfs.append(api_data)\n",
    "        \n",
    "        return pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    def _fetch_esg_metrics(self) -> pd.DataFrame:\n",
    "        \"\"\"Fetch ESG data from various sources\"\"\"\n",
    "        \n",
    "        # MongoDB collection for ESG metrics\n",
    "        esg_collection = self.mongo_client.green_finance.esg_metrics\n",
    "        \n",
    "        # Fetch from multiple sources\n",
    "        esg_data = []\n",
    "        \n",
    "        # 1. Carbon Emissions Data\n",
    "        emissions_data = pd.DataFrame(list(esg_collection.find(\n",
    "            {'metric_type': 'emissions'}\n",
    "        )))\n",
    "        \n",
    "        # 2. Social Impact Metrics\n",
    "        social_data = pd.DataFrame(list(esg_collection.find(\n",
    "            {'metric_type': 'social_impact'}\n",
    "        )))\n",
    "        \n",
    "        # 3. Governance Scores\n",
    "        governance_data = pd.DataFrame(list(esg_collection.find(\n",
    "            {'metric_type': 'governance'}\n",
    "        )))\n",
    "        \n",
    "        return pd.concat([emissions_data, social_data, governance_data], \n",
    "                        ignore_index=True)\n",
    "    \n",
    "    def _fetch_climate_data(self) -> pd.DataFrame:\n",
    "        \n",
    "        climate_data = []\n",
    "        \n",
    "        # 1. Weather Data APIs\n",
    "        for weather_api in self.config['weather_apis']:\n",
    "            response = requests.get(weather_api['endpoint'])\n",
    "            if response.status_code == 200:\n",
    "                climate_data.append(pd.DataFrame(response.json()))\n",
    "        \n",
    "        # 2. Environmental Impact Assessments\n",
    "        env_impact_query = \"\"\"\n",
    "        SELECT project_id, impact_type, impact_score\n",
    "        FROM environmental_impacts\n",
    "        WHERE assessment_date >= NOW() - INTERVAL '6 months'\n",
    "        \"\"\"\n",
    "        env_impacts = pd.read_sql(env_impact_query, self.sql_engine)\n",
    "        climate_data.append(env_impacts)\n",
    "        \n",
    "        return pd.concat(climate_data, ignore_index=True)\n",
    "    \n",
    "    def preprocess_data(self, structured_data: pd.DataFrame, \n",
    "                       unstructured_data: Dict) -> Dict:\n",
    "        \"\"\"Preprocess and clean the data\"\"\"\n",
    "        \n",
    "        # 1. Handle missing values\n",
    "        structured_data = self._handle_missing_values(structured_data)\n",
    "        \n",
    "        # 2. Feature engineering\n",
    "        structured_data = self._engineer_features(structured_data)\n",
    "        \n",
    "        # 3. Text preprocessing for unstructured data\n",
    "        processed_docs = self._preprocess_documents(unstructured_data)\n",
    "        \n",
    "        # 4. Normalize numerical features\n",
    "        structured_data = self._normalize_features(structured_data)\n",
    "        \n",
    "        return {\n",
    "            'structured_data': structured_data,\n",
    "            'processed_documents': processed_docs\n",
    "        }\n",
    "    \n",
    "    def validate_data(self, data: Dict) -> bool:\n",
    "        \"\"\"Validate data quality and consistency\"\"\"\n",
    "        \n",
    "        validation_results = []\n",
    "        \n",
    "        # 1. Check for required fields\n",
    "        required_fields = self.config['required_fields']\n",
    "        fields_present = all(field in data['structured_data'].columns \n",
    "                           for field in required_fields)\n",
    "        validation_results.append(fields_present)\n",
    "        \n",
    "        # 2. Validate data types\n",
    "        correct_types = self._validate_data_types(data['structured_data'])\n",
    "        validation_results.append(correct_types)\n",
    "        \n",
    "        # 3. Check value ranges\n",
    "        valid_ranges = self._validate_value_ranges(data['structured_data'])\n",
    "        validation_results.append(valid_ranges)\n",
    "        \n",
    "        return all(validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, pipeline: DataPipeline):\n",
    "        self.pipeline = pipeline\n",
    "    \n",
    "    def load_data(self) -> Dict:\n",
    "        \"\"\"Main method to load and prepare all data\"\"\"\n",
    "        \n",
    "        # 1. Ingest all data\n",
    "        structured_data = self.pipeline.ingest_structured_data()\n",
    "        unstructured_data = self.pipeline.ingest_unstructured_data()\n",
    "        \n",
    "        # 2. Preprocess data\n",
    "        processed_data = self.pipeline.preprocess_data(\n",
    "            structured_data, unstructured_data\n",
    "        )\n",
    "        \n",
    "        # 3. Validate data\n",
    "        if not self.pipeline.validate_data(processed_data):\n",
    "            raise ValueError(\"Data validation failed\")\n",
    "        \n",
    "        return processed_data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
